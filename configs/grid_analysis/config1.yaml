# Full training set for both training and fine tuning with same noise seed
training_seed: &training_seed 3
dataset_seed: &dataset_seed 3


strategy:
  finetuning_set: &finetuning_set 'Train'
  noise:
    pretraining:
      - set: 'Train'
        noise_rate: 0.0
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.1
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.2
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.3
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.4
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.5
        noise_type: 'symmetric'
        seed: 8

    finetuning:
      - set: 'Train'
        noise_rate: 0.1
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.2
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.3
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.4
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.5
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.6
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.7
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.8
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 0.9
        noise_type: 'symmetric'
        seed: 8
      - set: 'Train'
        noise_rate: 1.0
        noise_type: 'symmetric'
        seed: 8
      


dataset:
  name: "cifar10"
  num_classes: &num_classes 10
  img_size: [32, 32]
  subsample_size: [-1, -1]
  class_subset: []
  remap_labels: False
  heldout_conf: null
  grayscale: False
  normalize_imgs: True
  flatten: False
  valset_ratio: 0.0
  num_workers: 4
  seed: *dataset_seed


model:
  type: 'cnn5'
  num_channels: 128
  num_classes: *num_classes
  loss_fn: 'CE'
  metrics: ['ACC', 'F1']

trainer:
  pretraining:
    max_epochs: 700
    batch_size: 1024
    optimizer_cfg: 
      type: "adam"
      lr: 1.0e-4
      betas: [0.9, 0.999]
    lr_schedule_cfg: null
    early_stopping: False
    validation_freq: -1
    save_best_model: False
    checkpoint_freq: -1
    run_on_gpu: True
    use_amp: True
    log_comet: True
    comet_project_name: 'task-vectors-cifar10-grid'
    seed: *training_seed

  finetuning:
    max_epochs: 500
    batch_size: 1024
    optimizer_cfg: 
      type: "adam"
      lr: 5.0e-5
      betas: [0.9, 0.999]
    lr_schedule_cfg: null
    early_stopping: False
    validation_freq: -1
    save_best_model: False
    checkpoint_freq: -1
    run_on_gpu: True
    use_amp: True
    log_comet: True
    comet_project_name: 'task-vectors-cifar10-grid'
    seed: *training_seed

