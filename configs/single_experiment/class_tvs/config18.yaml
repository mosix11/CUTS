# Config 15 with CNN5 no norm
# The goal of this config is to check the whether BN layers are causing the problem of model collapsing and outputting only class 0 in the
# Config 15 or not. As we saw in configs 16 and 17, where we changed the model from CNN5 to FC1, the model did not collapse and
# we could use scaling factors greater than 0.1 when adding all task vectors to the pretrained model. So the problem seemed to stem from
# BN layers.

# The results of config 18 showed that the problem was in fact stemming from BN layers. However we don't still get orthogonal task vectors
# and the cosine similarity of the task vectors associatiated to different classes ar not close to zero (class 0 to other classes between
# 0.2 to 0.3). Let's try changing `finetuning_set = 'Heldout+Train'` to `finetuning_set = 'Heldout'` and see if we get more orthogonal 
# task vectors in config 19.

training_seed: &training_seed 11
dataset_seed: &dataset_seed 11


strategy:
  methodology: 'ClassTVFreezeHead'
  finetuning_set: &finetuning_set 'Heldout+Train'
  
  # noise:
  #   pretraining:
  #     set: 'Train'
  #     noise_rate: 0.0
  #     noise_type: 'symmetric'
  #     seed: 8
  #   finetuning:
  #     set: *finetuning_set
  #     noise_rate: 1.0
  #     noise_type: 'symmetric'
  #     seed: 8


dataset:
  name: "cifar10"
  num_classes: &num_classes 10
  img_size: [32, 32]
  subsample_size: [-1, -1]
  class_subset: []
  remap_labels: False
  heldout_conf: 
    0: 0.85
    1: 0.85
    2: 0.85
    3: 0.85
    4: 0.85
    5: 0.85
    6: 0.85
    7: 0.85
    8: 0.85
    9: 0.85
  grayscale: False
  normalize_imgs: True
  flatten: False
  valset_ratio: 0.0
  num_workers: 8
  seed: *dataset_seed


model:
  type: 'cnn5_nonorm'
  num_channels: 128
  num_classes: *num_classes
  loss_fn: 'CE'
  metrics: ['ACC', 'F1']


trainer:
  pretraining:
    max_epochs: 600
    batch_size: 1024
    optimizer_cfg: 
      type: "adam"
      lr: 1.0e-4
      betas: [0.9, 0.999]
    lr_schedule_cfg: null
    early_stopping: False
    validation_freq: -1
    save_best_model: False
    checkpoint_freq: -1
    run_on_gpu: True
    use_amp: True
    log_comet: True
    comet_project_name: 'task-vectors-cifar10' # change to class-task-vectors-cifa10 for new trainings
    seed: *training_seed

  finetuning:
    max_epochs: 400
    batch_size: 1024
    optimizer_cfg: 
      type: "adam"
      lr: 1.0e-5
      betas: [0.9, 0.999]
    lr_schedule_cfg: null
    early_stopping: False
    validation_freq: -1
    save_best_model: False
    checkpoint_freq: -1
    run_on_gpu: True
    use_amp: True
    log_comet: True
    comet_project_name: 'task-vectors-cifar10' # change to class-task-vectors-cifa10 for new trainings
    seed: *training_seed

