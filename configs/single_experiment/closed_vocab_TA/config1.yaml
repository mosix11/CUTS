training_seed: &training_seed 11
dataset_seed: &dataset_seed 11

datasets:
  - name: "mnist"
    num_classes: 10
    img_size: [224, 224]
    batch_size: 128
    valset_ratio: 0.0
    num_workers: 8
    seed: *dataset_seed

  - name: "fashion_mnist"
    num_classes: 128
    img_size: [224, 224]
    batch_size: 64
    valset_ratio: 0.0
    num_workers: 8
    seed: *dataset_seed

  - name: "cifar10"
    num_classes: 128
    img_size: [224, 224]
    batch_size: 128
    valset_ratio: 0.0
    num_workers: 8
    seed: *dataset_seed

  - name: "cifar100"
    num_classes: 100
    img_size: [224, 224]
    batch_size: 128
    valset_ratio: 0.0
    num_workers: 8
    seed: *dataset_seed

  - name: "food101"
    num_classes: 101
    img_size: [224, 224]
    batch_size: 128
    valset_ratio: 0.0
    num_workers: 8
    seed: *dataset_seed

  - name: "flowers102"
    num_classes: 102
    img_size: [224, 224]
    batch_size: 128
    valset_ratio: 0.0
    num_workers: 8
    seed: *dataset_seed

  - name: "kmnist"
    num_classes: 10
    img_size: [224, 224]
    batch_size: 128
    valset_ratio: 0.0
    num_workers: 8
    seed: *dataset_seed


model:
  type: 'open_clip_multi_head_ViT-B-16'
  pt_weights: 'datacomp_xl_s13b_b90k'
  heads_cfg:
    - head_name: 'mnist'
      head_out_dim: 10
    - head_name: 'fashion_mnist'
      head_out_dim: 10
    - head_name: 'cifar10'
      head_out_dim: 10
    - head_name: 'cifar100'
      head_out_dim: 100
    - head_name: 'food101'
      head_out_dim: 101
    - head_name: 'food102'
      head_out_dim: 102
    - head_name: 'kmnist'
      head_out_dim: 10

  loss_fn: 'CE'
  metrics: ['ACC', 'F1']


trainer:
  linear_probing:
    max_iterations: 8000
    optimizer_cfg: 
      type: "adamw"
      lr: 1.0e-4
      betas: [0.9, 0.999]
    lr_schedule_cfg:
      type: 'cosann'
      update_on: 'gradient_step'
      T_max: 8000
      eta_min: 0
    early_stopping: False
    validation_freq: -1
    save_best_model: False
    checkpoint_freq: -1
    run_on_gpu: True
    use_amp: True
    log_comet: True
    comet_project_name: 'closed_vocab_TA'
    seed: *training_seed

  finetuning:
    max_iterations: 2000
    optimizer_cfg: 
      type: "adamw"
      lr: 1.0e-5
      betas: [0.9, 0.999]
    lr_schedule_cfg:
      type: 'cosann_warmup'
      update_on: 'gradient_step'
      warmup_steps: 200
      T_max: 2000
      eta_min: 0.0
    early_stopping: False
    validation_freq: -1
    save_best_model: False
    checkpoint_freq: -1
    run_on_gpu: True
    use_amp: True
    log_comet: True
    comet_project_name: 'closed_vocab_TA'
    seed: *training_seed

